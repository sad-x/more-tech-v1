# MoreTech SCD2 Loader

## Проблематика

В озере данных ВТБ необходимо хранить полную историю изменений из оперативного хранилища данных, поступающую в формате SCD2.
При обработке новых сущностей записи добавляются в реплику. Но при обновлении существующих сущностей требуется изменить дату окончания действия у предыдущей записи по идентификатору сущности.
Выбранная архитектура озера данных не поддерживает операции обновления. Возникает проблема эффективной обработки инкрементальных данных и корректного обновления реплики в приемлемые для пользователей сроки.

## Решение

- Движок для обработки данных: Apache Spark.
- Формат хранения данных: Apache Parquet.

Алгоритм:
1. Добавление полей партицирования eff_from_month и eff_to_month: последние дни месяца даты начала и окончания действия записи. 
2. Партицирование инкремента по добавленным полям.
3. Запись инкремента во временную таблицу.
4. Подготовка вспомогательного датафрейма из ключевых полей (id, eff_from_dt) уникальных закрытых записей инкремента
5. Поиск открытых записей в реплике, которые не требуют обновления, с последующим добавлением их во временную таблицу. Реализовано через anti join реплики и вспомогательного датафрейма из предыдущего пункта. Обработка происходит в цикле для каждой субпартиции eff_from_month в реплике для партиции с открытыми записями, где eff_to_month=5999-12-31. Данный этап выполняется в несколько потоков для улучшения производительности.
6. Удаление открытых записей из реплики 
7. Запись всей временной таблицы в реплику

Особенности решения:
- Реализовано логирование каждого запуска с замерами скорости по каждому этапу для более удобного отслеживания хода работы и производительности
- Конфигурация модуля выполняется посредством .ini файла
- Для большей гибкости решения предоставляется возможность запуска с разным количеством параллельно работающих потоков. Наилучшего результата на **среднем** датасете (level=3) на ресурсах тестового стенда удалось добиться при запуске конфигурации с 4 потоками

## Запуск

level - вариант тестируемых данных:
- 2 - маленький набор данных (10млн-1млн)
- 3 - средний набор данных (100млн-10млн)
- 4 - большой набор данных (1млрд-100млн)

**Внимание!**
В целях безопасности адреса и ключи S3-бакетов заменены заглушками вида <ACCESS_KEY> в самих скриптах и в конфигурационном файле. 
Пожалуйста убедитесь, что перед запуском заглушки были заменены на реальные данные для авторизации.

1. Инициализация данных

```commandline
/opt/spark/bin/spark-submit --jars ~/spark-hadoop-cloud_2.13-3.5.3.jar src/copy_init.py -l <level>
```

2. Запуск применения инкремента

```commandline
/opt/spark/bin/spark-submit --jars ~/spark-hadoop-cloud_2.13-3.5.3.jar src/join.py -l <level>
```

3. Проверка тестируемых данных

```commandline
/opt/spark/bin/spark-submit --jars ~/spark-hadoop-cloud_2.13-3.5.3.jar src/check_result.py -l <level>
```